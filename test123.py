import os
import google.generativeai as genai
import pandas as pd
import streamlit as st
from google.generativeai.types import HarmCategory, HarmBlockThreshold
import dotenv

# --- 1. ‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏´‡∏ô‡πâ‡∏≤‡∏à‡∏≠‡πÅ‡∏•‡∏∞‡∏™‡πÑ‡∏ï‡∏•‡πå (Theme: Tech & Modern) ---
st.set_page_config(page_title="CompTech AI - ‡∏ú‡∏π‡πâ‡∏ä‡πà‡∏ß‡∏¢‡∏Ñ‡∏≠‡∏°‡∏û‡∏¥‡∏ß‡πÄ‡∏ï‡∏≠‡∏£‡πå", page_icon="üíª", layout="centered")

st.markdown("""
    <style>
    /* ‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏á‡∏û‡∏∑‡πâ‡∏ô‡∏´‡∏•‡∏±‡∏á‡πÅ‡∏•‡∏∞‡∏ü‡∏≠‡∏ô‡∏ï‡πå */
    .stApp {
        background-color: #0e1117;
        color: #ffffff;
    }
    /* ‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏á‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠ */
    h1 {
        color: #00d4ff;
        font-family: 'Courier New', Courier, monospace;
        text-align: center;
        text-shadow: 2px 2px 4px #000000;
    }
    /* ‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏á‡∏Å‡∏•‡πà‡∏≠‡∏á‡πÅ‡∏ä‡∏ó */
    .stChatMessage {
        border-radius: 15px;
        margin-bottom: 10px;
    }
    /* ‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏á‡∏õ‡∏∏‡πà‡∏°‡πÉ‡∏ô Sidebar */
    .stButton>button {
        width: 100%;
        border-radius: 10px;
        border: 1px solid #00d4ff;
        background-color: transparent;
        color: #00d4ff;
        transition: 0.3s;
    }
    .stButton>button:hover {
        background-color: #00d4ff;
        color: #0e1117;
    }
    </style>
    """, unsafe_allow_html=True)

# --- 2. ‡πÇ‡∏´‡∏•‡∏î‡∏Ñ‡πà‡∏≤ Environment ---
dotenv.load_dotenv()
GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')

if not GOOGLE_API_KEY:
    st.error("üîë ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ API Key ‡πÉ‡∏ô‡∏£‡∏∞‡∏ö‡∏ö‡∏Å‡πà‡∏≠‡∏ô‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô")
    st.stop()

genai.configure(api_key=GOOGLE_API_KEY)

# --- 3. ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Model (Gemini 2.5 Flash) ---
model_name = "gemini-2.5-flash" 

generation_config = {
    "temperature": 0.4, # ‡∏õ‡∏£‡∏±‡∏ö‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ AI ‡∏ä‡πà‡∏ß‡∏¢‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡πÑ‡∏î‡πâ‡∏¢‡∏∑‡∏î‡∏´‡∏¢‡∏∏‡πà‡∏ô‡∏Ç‡∏∂‡πâ‡∏ô
    "top_p": 0.9,
    "max_output_tokens": 2048,
}

# ‡∏Å‡∏≥‡∏´‡∏ô‡∏î System Instruction ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏ú‡∏π‡πâ‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç‡∏Ñ‡∏≠‡∏°‡∏û‡∏¥‡∏ß‡πÄ‡∏ï‡∏≠‡∏£‡πå
SYSTEM_PROMPT = """‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠ 'CompTech AI' ‡∏ú‡∏π‡πâ‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç‡∏î‡πâ‡∏≤‡∏ô‡∏Ñ‡∏≠‡∏°‡∏û‡∏¥‡∏ß‡πÄ‡∏ï‡∏≠‡∏£‡πå ‡∏Æ‡∏≤‡∏£‡πå‡∏î‡πÅ‡∏ß‡∏£‡πå ‡∏ã‡∏≠‡∏ü‡∏ï‡πå‡πÅ‡∏ß‡∏£‡πå ‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏° 
‡∏ö‡∏∏‡∏Ñ‡∏•‡∏¥‡∏Å‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠: ‡∏â‡∏•‡∏≤‡∏î, ‡∏ó‡∏±‡∏ô‡∏™‡∏°‡∏±‡∏¢, ‡πÅ‡∏•‡∏∞‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏¢‡∏≤‡∏Å‡πÉ‡∏´‡πâ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏á‡πà‡∏≤‡∏¢ 
- ‡∏´‡∏≤‡∏Å‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå ‡πÉ‡∏´‡πâ‡∏¢‡∏∂‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
- ‡∏´‡∏≤‡∏Å‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡∏î‡πâ‡∏≤‡∏ô‡πÑ‡∏≠‡∏ó‡∏µ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡∏ï‡∏≠‡∏ö‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏°‡∏∑‡∏≠‡∏≠‡∏≤‡∏ä‡∏µ‡∏û
- ‡∏û‡∏π‡∏î‡∏à‡∏≤‡∏™‡∏∏‡∏†‡∏≤‡∏û ‡πÅ‡∏•‡∏∞‡πÉ‡∏ä‡πâ‡∏Ñ‡∏≥‡∏®‡∏±‡∏û‡∏ó‡πå‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°"""

model = genai.GenerativeModel(
    model_name=model_name,
    generation_config=generation_config,
    system_instruction=SYSTEM_PROMPT
)

# --- 4. ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (Knowledge Base) ---
@st.cache_data
def load_context(path):
    try:
        if os.path.exists(path):
            df = pd.read_excel(path, engine='openpyxl')
            return df.to_string(index=False)
        return None
    except Exception as e:
        return None

# ‡πÑ‡∏ü‡∏•‡πå Excel ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏Å‡πá‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ó‡∏≤‡∏á (‡πÄ‡∏ä‡πà‡∏ô ‡∏£‡∏≤‡∏Ñ‡∏≤‡∏≠‡∏∏‡∏õ‡∏Å‡∏£‡∏ì‡πå, ‡∏ß‡∏¥‡∏ò‡∏µ‡∏ã‡πà‡∏≠‡∏°‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏≠‡∏≤‡∏Å‡∏≤‡∏£)
file_path = "Computer_Context.xlsx" 
file_content = load_context(file_path)

# --- 5. ‡∏™‡πà‡∏ß‡∏ô Sidebar ---
with st.sidebar:
    st.markdown("## ‚öôÔ∏è IT Support Center")
    st.image("https://cdn-icons-png.flaticon.com/512/2001/2001405.png", width=80)
    st.info("‡∏™‡∏≠‡∏ö‡∏ñ‡∏≤‡∏°‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á: ‡∏à‡∏±‡∏î‡∏™‡πÄ‡∏õ‡∏Å‡∏Ñ‡∏≠‡∏°, ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç Windows, ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡πÇ‡∏Ñ‡πâ‡∏î ‡∏´‡∏£‡∏∑‡∏≠‡∏≠‡∏±‡∏õ‡πÄ‡∏Å‡∏£‡∏î‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á")
    
    if st.button("üóëÔ∏è Clear Terminal Cache"):
        st.session_state["messages"] = [
            {"role": "assistant", "content": "‡∏£‡∏∞‡∏ö‡∏ö Reboot ‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢... ‡∏°‡∏µ‡∏≠‡∏∞‡πÑ‡∏£‡πÉ‡∏´‡πâ CompTech ‡∏ä‡πà‡∏ß‡∏¢‡∏î‡∏π‡πÅ‡∏•‡∏Ñ‡∏£‡∏±‡∏ö?"}
        ]
        st.rerun()
    
    st.divider()
    st.caption("System Status: Online")
    st.caption("Core: Gemini 2.5 Flash")

# --- 6. ‡∏™‡πà‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡πÅ‡∏ä‡∏ó ---
st.title("ü§ñ CompTech AI Support")
st.caption("Professional Computer & Technology Assistant")

if "messages" not in st.session_state:
    st.session_state["messages"] = [
        {"role": "assistant", "content": "‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ‡∏Ñ‡∏£‡∏±‡∏ö ‡∏ú‡∏° CompTech AI ‡∏¢‡∏¥‡∏ô‡∏î‡∏µ‡∏ï‡πâ‡∏≠‡∏ô‡∏£‡∏±‡∏ö‡πÄ‡∏Ç‡πâ‡∏≤‡∏™‡∏π‡πà‡∏®‡∏π‡∏ô‡∏¢‡πå‡∏ä‡πà‡∏ß‡∏¢‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡∏î‡πâ‡∏≤‡∏ô‡∏Ñ‡∏≠‡∏°‡∏û‡∏¥‡∏ß‡πÄ‡∏ï‡∏≠‡∏£‡πå ‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏´‡∏£‡∏∑‡∏≠‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∂‡∏Å‡∏©‡∏≤‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏™‡πÄ‡∏õ‡∏Å‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡πÑ‡∏´‡∏°‡∏Ñ‡∏£‡∏±‡∏ö?"}
    ]

# ‡πÅ‡∏™‡∏î‡∏á‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏Ñ‡∏∏‡∏¢
for msg in st.session_state["messages"]:
    with st.chat_message(msg["role"]):
        st.write(msg["content"])

# ‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°
if prompt := st.chat_input("‡∏™‡∏≠‡∏ö‡∏ñ‡∏≤‡∏°‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏Ñ‡∏≠‡∏°‡∏û‡∏¥‡∏ß‡πÄ‡∏ï‡∏≠‡∏£‡πå... (‡πÄ‡∏ä‡πà‡∏ô ‡∏Ñ‡∏≠‡∏°‡πÄ‡∏õ‡∏¥‡∏î‡πÑ‡∏°‡πà‡∏ï‡∏¥‡∏î‡∏ó‡∏≥‡πÑ‡∏á?)"):
    st.session_state["messages"].append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.write(prompt)

    with st.chat_message("assistant"):
        with st.spinner("Analyzing system status..."):
            try:
                history = []
                # ‡πÉ‡∏™‡πà Context (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)
                if file_content:
                    history.append({"role": "user", "parts": [f"Technical Knowledge Base: {file_content}"]})
                    history.append({"role": "model", "parts": ["‡∏£‡∏±‡∏ö‡∏ó‡∏£‡∏≤‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô‡∏Ñ‡∏£‡∏±‡∏ö"]})
                
                # ‡∏î‡∏∂‡∏á‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î
                for msg in st.session_state["messages"][-6:]:
                    role = "model" if msg["role"] == "assistant" else "user"
                    history.append({"role": role, "parts": [msg["content"]]})

                chat_session = model.start_chat(history=history)
                response = chat_session.send_message(prompt)
                response_text = response.text
                
            except Exception as e:
                response_text = f"‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢‡∏Ñ‡∏£‡∏±‡∏ö ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏£‡∏∞‡∏ö‡∏ö‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•: {str(e)}"

            st.write(response_text)
            st.session_state["messages"].append({"role": "assistant", "content": response_text})
